import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from PIL import Image
import os
from tqdm import tqdm
import cv2
import hdbscan
import umap
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns
import gc
from itertools import product
import warnings
warnings.filterwarnings('ignore')

# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
def clear_gpu_memory():
    gc.collect()
    torch.cuda.empty_cache()

# ============================================================================
# Positional Encoding 
# ============================================================================

def get_2d_positional_encoding(x, y, dim=64, max_len=10000):
    """
    2D sin/cos Positional Encoding
    
    Args:
        x, y: normalized coordinates (0~1 range)
        dim: encoding dimension (must be divisible by 4)
    """
    assert dim % 4 == 0, "dim must be divisible by 4 for 2D encoding"
    
    pos_encoding = np.zeros((len(x), dim))
    d_model_quarter = dim // 4
    div_term = np.exp(np.arange(0, d_model_quarter) * -(np.log(max_len) / d_model_quarter))
    
    # X coordinate encoding
    pos_encoding[:, 0::4] = np.sin(x[:, None] * div_term)
    pos_encoding[:, 1::4] = np.cos(x[:, None] * div_term)
    
    # Y coordinate encoding  
    pos_encoding[:, 2::4] = np.sin(y[:, None] * div_term)
    pos_encoding[:, 3::4] = np.cos(y[:, None] * div_term)
    
    return pos_encoding

def create_positional_features(df, pos_dim=64):
    """
    Create positional encoding from array coordinates (tissueë³„ ìƒëŒ€ì¢Œí‘œ)
    """
    print("Creating positional encoding features...")
    
    pos_features_list = []
    
    # Tissueë³„ë¡œ ì •ê·œí™”
    for tissue_id in df['tissue_index'].unique():
        tissue_mask = df['tissue_index'] == tissue_id
        tissue_data = df[tissue_mask]
        
        # Array ì¢Œí‘œ ì •ê·œí™” (0~1)
        x_coords = tissue_data['array_row'].values
        y_coords = tissue_data['array_col'].values
        
        x_min, x_max = x_coords.min(), x_coords.max()
        y_min, y_max = y_coords.min(), y_coords.max()
        
        if x_max > x_min and y_max > y_min:
            x_norm = (x_coords - x_min) / (x_max - x_min)
            y_norm = (y_coords - y_min) / (y_max - y_min)
        else:
            x_norm = np.zeros_like(x_coords)
            y_norm = np.zeros_like(y_coords)
        
        # Positional encoding ìƒì„±
        tissue_pos = get_2d_positional_encoding(x_norm, y_norm, dim=pos_dim)
        pos_features_list.append(tissue_pos)
    
    pos_features = np.concatenate(pos_features_list)
    
    print(f" Positional encoding created: {pos_features.shape}")
    return pos_features

# ============================================================================
# Dataset & Feature Extractors
# ============================================================================

class TissueDataset(Dataset):
    def __init__(self, image_dir, df, transform):
        self.image_dir = image_dir
        self.transform = transform
        self.df = df
        
        # IDì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ëª…
        self.image_files = [f"{row['id']}.png" for _, row in df.iterrows()]
        print(f"Dataset loaded: {len(self.image_files)} images")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        image = Image.open(os.path.join(self.image_dir, img_name))
        
        if self.transform:
            image = self.transform(image)
            
        return image, img_name

# ============================================================================
# ì¡°ì§ë³‘ë¦¬ íŠ¹í™” ResNet18 ë¡œë”
# ============================================================================

def load_histology_resnet18(model_path='weights/tenpercent_resnet18.ckpt', device='cuda'):
    """
    ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet18 ëª¨ë¸ ë¡œë“œ
    """
    print(f" Loading histology-specialized ResNet18 from {model_path}")
    
    def load_model_weights(model, weights):
        model_dict = model.state_dict()
        weights = {k: v for k, v in weights.items() if k in model_dict}
        if weights == {}:
            print(' No weight could be loaded..')
        model_dict.update(weights)
        model.load_state_dict(model_dict)
        return model
    
    # ResNet18 ëª¨ë¸ ìƒì„± (pretrained=False)
    model = models.resnet18(pretrained=False)
    
    try:
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        state = torch.load(model_path, map_location=device)
        state_dict = state['state_dict']
        
        # í‚¤ ì´ë¦„ ì •ë¦¬ (model., resnet. ì œê±°)
        for key in list(state_dict.keys()):
            state_dict[key.replace('model.', '').replace('resnet.', '')] = state_dict.pop(key)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model = load_model_weights(model, state_dict)
        
        # FC layerë¥¼ Identityë¡œ ë³€ê²½ (íŠ¹ì§• ì¶”ì¶œìš©)
        model.fc = nn.Identity()
        
        model = model.to(device)
        model.eval()
        
        print(" Histology-specialized ResNet18 loaded successfully!")
        return model
        
    except Exception as e:
        print(f" Error loading histology weights: {e}")
        print(" Falling back to ImageNet pretrained ResNet18...")
        
        # ImageNet ì‚¬ì „í›ˆë ¨ ëª¨ë¸ë¡œ í´ë°±
        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        model.fc = nn.Identity()
        model = model.to(device)
        model.eval()
        
        return model

class HistologyFeatureExtractor:
    def __init__(self, model_path='weights/tenpercent_resnet18.ckpt', 
                 device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        print(f"Using device: {device}")
        
        # ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet18 ë¡œë“œ
        self.model = load_histology_resnet18(model_path, device)

    def extract_features(self, dataloader):
        """ì¡°ì§ë³‘ë¦¬í•™ ResNet18 íŠ¹ì§• ì¶”ì¶œ"""
        resnet_features = []
        names = []
        
        with torch.no_grad():
            for images, img_names in tqdm(dataloader, desc="Extracting histology features"):
                images = images.to(self.device)
                
                # ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet íŠ¹ì§•
                batch_resnet = self.model(images)
                
                resnet_features.append(batch_resnet.cpu().numpy())
                names.extend(img_names)
                
                if len(resnet_features) % 10 == 0:
                    clear_gpu_memory()
        
        resnet_features = np.concatenate(resnet_features)
        
        print(f" Histology feature extraction complete:")
        print(f"   - ResNet features: {resnet_features.shape}")
        
        return resnet_features, names

# ============================================================================
#  Feature Fusion & Clustering
# ============================================================================

def fuse_features(resnet_features, pos_features, 
                 resnet_weight=1.0, pos_weight=100.0):
    """
    íŠ¹ì§• ìœµí•©: Histology ResNet + Positional Encoding
    """
    # L2 ì •ê·œí™” í›„ ê°€ì¤‘ì¹˜ ì ìš©
    resnet_norm = resnet_features / np.linalg.norm(resnet_features, axis=1, keepdims=True)
    pos_norm = pos_features / np.linalg.norm(pos_features, axis=1, keepdims=True)
    
    # ê°€ì¤‘ì¹˜ ì ìš© í›„ ê²°í•©
    fused_features = np.concatenate([
        resnet_norm * resnet_weight,
        pos_norm * pos_weight
    ], axis=1)
    
    return fused_features

def evaluate_clustering(features, clusters):
    """í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ í‰ê°€"""
    if len(set(clusters)) <= 1:  # í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œ ì´í•˜ë©´ í‰ê°€ ë¶ˆê°€
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': 0}
    
    # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì œê±°
    valid_mask = clusters != -1
    if np.sum(valid_mask) < 2:
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': 0}
    
    valid_features = features[valid_mask]
    valid_clusters = clusters[valid_mask]
    
    if len(set(valid_clusters)) <= 1:
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': len(set(valid_clusters))}
    
    try:
        silhouette = silhouette_score(valid_features, valid_clusters)
        calinski = calinski_harabasz_score(valid_features, valid_clusters)
        davies = davies_bouldin_score(valid_features, valid_clusters)
        n_clusters = len(set(valid_clusters))
        
        return {
            'silhouette': silhouette,
            'calinski': calinski,
            'davies': davies,
            'n_clusters': n_clusters
        }
    except:
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': 0}

def perform_clustering_with_eval(features):
    """UMAP + HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ë° í‰ê°€"""
    # íŠ¹ì§• ì •ê·œí™”
    features_scaled = StandardScaler().fit_transform(features)
    
    # UMAP ì°¨ì› ì¶•ì†Œ
    reducer = umap.UMAP(
        n_neighbors=10,
        min_dist=0.001,
        n_components=2,
        random_state=42,
        metric='euclidean'
    )
    umap_embedded = reducer.fit_transform(features_scaled)
    
    # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=70,
        min_samples=3,
        cluster_selection_epsilon=0.05,
        metric='euclidean'
    )
    clusters = clusterer.fit_predict(umap_embedded)
    
    # ì„±ëŠ¥ í‰ê°€
    metrics = evaluate_clustering(umap_embedded, clusters)
    
    return umap_embedded, clusters, metrics

# ============================================================================
#  ê°€ì¤‘ì¹˜ ì¡°í•© ìµœì í™” ë° ì‹œê°í™”
# ============================================================================

def comprehensive_weight_optimization(resnet_features, pos_features, save_dir='.'):
    """
    ê°€ì¤‘ì¹˜ ì¡°í•©ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ë° UMAP ì‹œê°í™”
    """
    print(" Starting weight optimization...")
    
    # 2ì°¨ì› ê°€ì¤‘ì¹˜ ì¡°í•© ì •ì˜
    resnet_weights = [0.1, 0.5, 1.0, 2.0, 5.0]
    pos_weights = [1.0, 10.0, 50.0, 100.0, 500.0]  
    
    print(f"Testing {len(resnet_weights)} Ã— {len(pos_weights)} = {len(resnet_weights) * len(pos_weights)} combinations")
    
    # ê²°ê³¼ ì €ì¥
    all_results = []
    
    combinations = list(product(resnet_weights, pos_weights))
    
    for i, (rw, pw) in enumerate(tqdm(combinations, desc="Testing weight combinations")):
        try:
            # íŠ¹ì§• ìœµí•©
            fused_features = fuse_features(resnet_features, pos_features,
                                         resnet_weight=rw, pos_weight=pw)
            
            # í´ëŸ¬ìŠ¤í„°ë§ ë° í‰ê°€
            umap_embedded, clusters, metrics = perform_clustering_with_eval(fused_features)
            
            result = {
                'combination_idx': i,
                'resnet_weight': rw,
                'pos_weight': pw,  
                'silhouette': metrics['silhouette'],
                'calinski': metrics['calinski'],
                'davies': metrics['davies'],
                'n_clusters': metrics['n_clusters'],
                'umap_embedded': umap_embedded,
                'clusters': clusters
            }
            
            all_results.append(result)
            
        except Exception as e:
            print(f"Error with combination ({rw}, {pw}): {e}")
            continue
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    results_df = pd.DataFrame([
        {k: v for k, v in r.items() if k not in ['umap_embedded', 'clusters']}
        for r in all_results
    ])
    
    # ê²°ê³¼ ì €ì¥
    results_df.to_csv(os.path.join(save_dir, 'weight_optimization_results.csv'), index=False)
    
    # ìµœì  ì¡°í•©ë“¤ ì„ íƒ
    top_results = sorted(all_results, key=lambda x: x['silhouette'], reverse=True)[:20]
    
    print(f"\n Top 20 combinations found!")
    print("="*60)
    for i, result in enumerate(top_results[:10]):
        print(f"{i+1:2d}. ResNet:{result['resnet_weight']:4.1f} | Pos:{result['pos_weight']:5.0f} | "
              f"Sil:{result['silhouette']:.3f} | Clusters:{result['n_clusters']:2d}")
    
    #  ì¢…í•© ì‹œê°í™”
    plot_comprehensive_results(top_results, results_df, save_dir)
    
    return top_results, results_df

def plot_comprehensive_results(top_results, results_df, save_dir):
    """ì¢…í•© ê²°ê³¼ ì‹œê°í™”"""
    
    # 1.  ìƒìœ„ 20ê°œ ì¡°í•©ì˜ UMAP ê²°ê³¼ (4x5 ê·¸ë¦¬ë“œ)
    fig, axes = plt.subplots(4, 5, figsize=(25, 20))
    axes = axes.flatten()
    
    for i, result in enumerate(top_results):
        ax = axes[i]
        
        umap_embedded = result['umap_embedded']
        clusters = result['clusters']
        
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
        noise_mask = clusters == -1
        if np.any(noise_mask):
            ax.scatter(umap_embedded[noise_mask, 0], umap_embedded[noise_mask, 1],
                      c='lightgray', alpha=0.3, s=8, label='Noise')
        
        # í´ëŸ¬ìŠ¤í„° í¬ì¸íŠ¸
        cluster_mask = ~noise_mask
        if np.any(cluster_mask):
            scatter = ax.scatter(umap_embedded[cluster_mask, 0], umap_embedded[cluster_mask, 1],
                               c=clusters[cluster_mask], cmap='tab20', alpha=0.8, s=12)
        
        ax.set_title(f'#{i+1}: ({result["resnet_weight"]:.1f}, {result["pos_weight"]:.0f})\n'
                    f'Sil: {result["silhouette"]:.3f} | Clusters: {result["n_clusters"]}', 
                    fontsize=10)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.grid(True, alpha=0.3)
    
    plt.suptitle(' Top 20 Weight Combinations - UMAP Clustering Results', fontsize=16, y=0.95)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'top20_umap_results.png'), dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # ì‹¤ë£¨ì—£ ì ìˆ˜ ë¶„í¬
    axes[0, 0].hist(results_df['silhouette'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].axvline(results_df['silhouette'].mean(), color='red', linestyle='--', 
                       label=f'Mean: {results_df["silhouette"].mean():.3f}')
    axes[0, 0].set_title('Silhouette Score Distribution')
    axes[0, 0].set_xlabel('Silhouette Score')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶„í¬
    max_clusters = results_df['n_clusters'].max()
    axes[0, 1].hist(results_df['n_clusters'], bins=range(0, max_clusters+2), 
                    alpha=0.7, color='lightgreen', edgecolor='black')
    axes[0, 1].set_title('Number of Clusters Distribution')
    axes[0, 1].set_xlabel('Number of Clusters')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].grid(True, alpha=0.3)
    
    # ResNet ê°€ì¤‘ì¹˜ë³„ ì„±ëŠ¥
    resnet_perf = results_df.groupby('resnet_weight')['silhouette'].agg(['mean', 'std']).reset_index()
    axes[0, 2].bar(resnet_perf['resnet_weight'], resnet_perf['mean'], 
                   yerr=resnet_perf['std'], capsize=5, alpha=0.7, color='orange')
    axes[0, 2].set_title('Performance by ResNet Weight')
    axes[0, 2].set_xlabel('ResNet Weight')
    axes[0, 2].set_ylabel('Mean Silhouette Score')
    axes[0, 2].grid(True, alpha=0.3)
    
    # Positional ê°€ì¤‘ì¹˜ë³„ ì„±ëŠ¥
    pos_perf = results_df.groupby('pos_weight')['silhouette'].agg(['mean', 'std']).reset_index()
    axes[1, 0].bar(range(len(pos_perf)), pos_perf['mean'], 
                   yerr=pos_perf['std'], capsize=5, alpha=0.7, color='red')
    axes[1, 0].set_title('Performance by Positional Weight')
    axes[1, 0].set_xlabel('Positional Weight')
    axes[1, 0].set_ylabel('Mean Silhouette Score')
    axes[1, 0].set_xticks(range(len(pos_perf)))
    axes[1, 0].set_xticklabels([f'{w:.0f}' for w in pos_perf['pos_weight']], rotation=45)
    axes[1, 0].grid(True, alpha=0.3)
    
    # ê°€ì¤‘ì¹˜ ì¡°í•© íˆíŠ¸ë§µ (ResNet vs Pos)
    pivot_data = results_df.groupby(['resnet_weight', 'pos_weight'])['silhouette'].mean().unstack()
    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='viridis', ax=axes[1, 1])
    axes[1, 1].set_title('Weight Combination Heatmap\n(ResNet vs Positional)')
    axes[1, 1].set_xlabel('Positional Weight')
    axes[1, 1].set_ylabel('ResNet Weight')
    
    # ë¹ˆ subplot ì œê±°
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'weight_optimization_analysis.png'), dpi=300, bbox_inches='tight')
    plt.show()

# ============================================================================
#  MAIN ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¬ Histology-Specialized Clustering with Weight Optimization")
    print("="*60)
    
    # ë°ì´í„° ë¡œë“œ
    print(" Loading data...")
    df = pd.read_csv('dataset.csv')
    
    #  ì¡°ì§ 3ë²ˆ ì œì™¸ (1, 2, 4ë²ˆë§Œ ì‚¬ìš©)
    print(f"Original data: {len(df)} samples")
    df = df[df['tissue_index'].isin([1, 2, 4])]
    print(f"After filtering (tissues 1,2,4 only): {len(df)} samples")
    print(f"Tissue distribution: {df['tissue_index'].value_counts().sort_index()}")
    
    # Transform ì„¤ì •
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Dataset & DataLoader
    dataset = TissueDataset('train', df, transform)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)
    
    #  1. Positional Encoding ìƒì„±
    print("\nStep 1: Creating positional encoding...")
    pos_features = create_positional_features(df, pos_dim=64)
    
    # 2. ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet íŠ¹ì§• ì¶”ì¶œ
    print("\n Step 2: Extracting histology-specialized features...")
    extractor = HistologyFeatureExtractor(model_path='weights/tenpercent_resnet18.ckpt')
    resnet_features, names = extractor.extract_features(dataloader)
    
    # 3. ê°€ì¤‘ì¹˜ ìµœì í™” ë° ì‹œê°í™”
    print("\n Step 3: Weight optimization...")
    top_results, results_df = comprehensive_weight_optimization(
        resnet_features, pos_features, save_dir='.'
    )
    
    #  4. ìµœì  ê²°ê³¼ë¡œ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
    print("\n Step 4: Final clustering with best weights...")
    best_result = top_results[0]
    
    #  5. ê²°ê³¼ ì €ì¥
    print("\n Step 5: Saving final results...")
    final_results_df = pd.DataFrame({
        'image_name': names,
        'cluster': best_result['clusters'],
        'array_row': df['array_row'].values,
        'array_col': df['array_col'].values,
        'tissue_index': df['tissue_index'].values,
        'umap_x': best_result['umap_embedded'][:, 0],
        'umap_y': best_result['umap_embedded'][:, 1]
    })
    
    final_results_df.to_csv('histology_clustering_results.csv', index=False)
    
    # ìµœì¢… í†µê³„
    cluster_stats = final_results_df['cluster'].value_counts().sort_index()
    print(f"\n Final Clustering Statistics (Best Combination):")
    print(f"   Best weights: ResNet={best_result['resnet_weight']:.1f}, "
          f"Pos={best_result['pos_weight']:.0f}")
    print(f"   Silhouette Score: {best_result['silhouette']:.3f}")
    print(f"   Number of clusters: {best_result['n_clusters']}")
    print("   Cluster distribution:")
    for cluster_id, count in cluster_stats.items():
        if cluster_id == -1:
            print(f"     Noise: {count} samples")
        else:
            print(f"     Cluster {cluster_id}: {count} samples")
    
    print("\nğŸ‰ Complete! Results saved:")
    print("   - histology_clustering_results.csv")
    print("   - weight_optimization_results.csv") 
    print("   - top20_umap_results.png")
    print("   - weight_optimization_analysis.png")

# ============================================================================
#  ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ============================================================================

def run_quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì†Œìˆ˜ ì¡°í•©ë§Œ)"""
    print(" Running Quick Test...")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('dataset.csv')
    
    #  ì¡°ì§ 3ë²ˆ ì œì™¸ (1, 2, 4ë²ˆë§Œ ì‚¬ìš©)  
    df = df[df['tissue_index'].isin([1, 2, 4])]
    print(f"Using tissues 1, 2, 4 only: {len(df)} samples")
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    dataset = TissueDataset('train', df, transform)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)
    
    # íŠ¹ì§• ì¶”ì¶œ
    pos_features = create_positional_features(df, pos_dim=64)
    extractor = HistologyFeatureExtractor(model_path='weights/tenpercent_resnet18.ckpt')
    resnet_features, names = extractor.extract_features(dataloader)
    
    # ì œí•œëœ ì¡°í•© í…ŒìŠ¤íŠ¸
    quick_combinations = [
        (0.5, 50), (1.0, 100), (2.0, 100), (1.0, 500)
    ]
    
    results = []
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    for i, (rw, pw) in enumerate(quick_combinations):
        print(f"Testing combination {i+1}/4: ({rw}, {pw})")
        
        fused_features = fuse_features(resnet_features, pos_features,
                                     resnet_weight=rw, pos_weight=pw)
        
        umap_embedded, clusters, metrics = perform_clustering_with_eval(fused_features)
        
        # ì‹œê°í™”
        ax = axes[i]
        noise_mask = clusters == -1
        if np.any(noise_mask):
            ax.scatter(umap_embedded[noise_mask, 0], umap_embedded[noise_mask, 1],
                      c='lightgray', alpha=0.3, s=10)
        
        cluster_mask = ~noise_mask
        if np.any(cluster_mask):
            ax.scatter(umap_embedded[cluster_mask, 0], umap_embedded[cluster_mask, 1],
                      c=clusters[cluster_mask], cmap='tab20', alpha=0.8, s=15)
        
        ax.set_title(f'Weights: ({rw}, {pw})\nSil: {metrics["silhouette"]:.3f} | '
                    f'Clusters: {metrics["n_clusters"]}')
        ax.set_xticks([])
        ax.set_yticks([])
        
        results.append((rw, pw, metrics['silhouette'], metrics['n_clusters']))
    
    plt.tight_layout()
    plt.savefig('quick_test_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n Quick Test Results:")
    results.sort(key=lambda x: x[2], reverse=True)
    for i, (rw, pw, sil, n_clust) in enumerate(results):
        print(f"{i+1}. ({rw:3.1f}, {pw:3.0f}) | Sil: {sil:.3f} | Clusters: {n_clust}")

if __name__ == "__main__":
    # ì‹¤í–‰ ì˜µì…˜ ì„ íƒ
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'quick':
        run_quick_test()
    else:
        # ê¸°ë³¸ ì‹¤í–‰
        main()

=================
def show_gallery(d, max_imgs=200, size=224):
    imgs = list_images(d)[:max_imgs]
    html = ["<div style='display:flex;flex-wrap:wrap;gap:6px'>"]
    for p in imgs:
        html.append(f"<img src='{p.as_posix()}' width='{size}' height='{size}'/>")
    html.append("</div>")
    display(HTML("".join(html)))

for d in clusters:
    print(d.name)
    show_gallery(d, max_imgs=300, size=224)  

import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from PIL import Image
import os
from tqdm import tqdm
import cv2
import hdbscan
import umap
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns
import gc
from itertools import product
import warnings
warnings.filterwarnings('ignore')

# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
def clear_gpu_memory():
    gc.collect()
    torch.cuda.empty_cache()

# ============================================================================
# Positional Encoding 
# ============================================================================

def get_2d_positional_encoding(x, y, dim=64, max_len=10000):
    """
    2D sin/cos Positional Encoding
    
    Args:
        x, y: normalized coordinates (0~1 range)
        dim: encoding dimension (must be divisible by 4)
    """
    assert dim % 4 == 0, "dim must be divisible by 4 for 2D encoding"
    
    pos_encoding = np.zeros((len(x), dim))
    d_model_quarter = dim // 4
    div_term = np.exp(np.arange(0, d_model_quarter) * -(np.log(max_len) / d_model_quarter))
    
    # X coordinate encoding
    pos_encoding[:, 0::4] = np.sin(x[:, None] * div_term)
    pos_encoding[:, 1::4] = np.cos(x[:, None] * div_term)
    
    # Y coordinate encoding  
    pos_encoding[:, 2::4] = np.sin(y[:, None] * div_term)
    pos_encoding[:, 3::4] = np.cos(y[:, None] * div_term)
    
    return pos_encoding

def create_positional_features(df, pos_dim=64):
    """
    Create positional encoding from array coordinates (tissueë³„ ìƒëŒ€ì¢Œí‘œ)
    """
    print("Creating positional encoding features...")
    
    pos_features_list = []
    
    # Tissueë³„ë¡œ ì •ê·œí™”
    for tissue_id in df['tissue_index'].unique():
        tissue_mask = df['tissue_index'] == tissue_id
        tissue_data = df[tissue_mask]
        
        # Array ì¢Œí‘œ ì •ê·œí™” (0~1)
        x_coords = tissue_data['array_row'].values
        y_coords = tissue_data['array_col'].values
        
        x_min, x_max = x_coords.min(), x_coords.max()
        y_min, y_max = y_coords.min(), y_coords.max()
        
        if x_max > x_min and y_max > y_min:
            x_norm = (x_coords - x_min) / (x_max - x_min)
            y_norm = (y_coords - y_min) / (y_max - y_min)
        else:
            x_norm = np.zeros_like(x_coords)
            y_norm = np.zeros_like(y_coords)
        
        # Positional encoding ìƒì„±
        tissue_pos = get_2d_positional_encoding(x_norm, y_norm, dim=pos_dim)
        pos_features_list.append(tissue_pos)
    
    pos_features = np.concatenate(pos_features_list)
    
    print(f" Positional encoding created: {pos_features.shape}")
    return pos_features

# ============================================================================
# Dataset & Feature Extractors
# ============================================================================

class TissueDataset(Dataset):
    def __init__(self, image_dir, df, transform):
        self.image_dir = image_dir
        self.transform = transform
        self.df = df
        
        # IDì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ëª…
        self.image_files = [f"{row['id']}.png" for _, row in df.iterrows()]
        print(f"Dataset loaded: {len(self.image_files)} images")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        image = Image.open(os.path.join(self.image_dir, img_name))
        
        if self.transform:
            image = self.transform(image)
            
        return image, img_name

# ============================================================================
# ì¡°ì§ë³‘ë¦¬ íŠ¹í™” ResNet18 ë¡œë”
# ============================================================================

def load_histology_resnet18(model_path='weights/tenpercent_resnet18.ckpt', device='cuda'):
    """
    ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet18 ëª¨ë¸ ë¡œë“œ
    """
    print(f" Loading histology-specialized ResNet18 from {model_path}")
    
    def load_model_weights(model, weights):
        model_dict = model.state_dict()
        weights = {k: v for k, v in weights.items() if k in model_dict}
        if weights == {}:
            print(' No weight could be loaded..')
        model_dict.update(weights)
        model.load_state_dict(model_dict)
        return model
    
    # ResNet18 ëª¨ë¸ ìƒì„± (pretrained=False)
    model = models.resnet18(pretrained=False)
    
    try:
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        state = torch.load(model_path, map_location=device)
        state_dict = state['state_dict']
        
        # í‚¤ ì´ë¦„ ì •ë¦¬ (model., resnet. ì œê±°)
        for key in list(state_dict.keys()):
            state_dict[key.replace('model.', '').replace('resnet.', '')] = state_dict.pop(key)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model = load_model_weights(model, state_dict)
        
        # FC layerë¥¼ Identityë¡œ ë³€ê²½ (íŠ¹ì§• ì¶”ì¶œìš©)
        model.fc = nn.Identity()
        
        model = model.to(device)
        model.eval()
        
        print(" Histology-specialized ResNet18 loaded successfully!")
        return model
        
    except Exception as e:
        print(f" Error loading histology weights: {e}")
        print(" Falling back to ImageNet pretrained ResNet18...")
        
        # ImageNet ì‚¬ì „í›ˆë ¨ ëª¨ë¸ë¡œ í´ë°±
        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        model.fc = nn.Identity()
        model = model.to(device)
        model.eval()
        
        return model

class HistologyFeatureExtractor:
    def __init__(self, model_path='weights/tenpercent_resnet18.ckpt', 
                 device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        print(f"Using device: {device}")
        
        # ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet18 ë¡œë“œ
        self.model = load_histology_resnet18(model_path, device)

    def extract_features(self, dataloader):
        """ì¡°ì§ë³‘ë¦¬í•™ ResNet18 íŠ¹ì§• ì¶”ì¶œ"""
        resnet_features = []
        names = []
        
        with torch.no_grad():
            for images, img_names in tqdm(dataloader, desc="Extracting histology features"):
                images = images.to(self.device)
                
                # ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet íŠ¹ì§•
                batch_resnet = self.model(images)
                
                resnet_features.append(batch_resnet.cpu().numpy())
                names.extend(img_names)
                
                if len(resnet_features) % 10 == 0:
                    clear_gpu_memory()
        
        resnet_features = np.concatenate(resnet_features)
        
        print(f" Histology feature extraction complete:")
        print(f"   - ResNet features: {resnet_features.shape}")
        
        return resnet_features, names

# ============================================================================
#  Feature Fusion & Clustering
# ============================================================================

def fuse_features(resnet_features, pos_features, 
                 resnet_weight=1.0, pos_weight=100.0):
    """
    íŠ¹ì§• ìœµí•©: Histology ResNet + Positional Encoding
    """
    # L2 ì •ê·œí™” í›„ ê°€ì¤‘ì¹˜ ì ìš©
    resnet_norm = resnet_features / np.linalg.norm(resnet_features, axis=1, keepdims=True)
    pos_norm = pos_features / np.linalg.norm(pos_features, axis=1, keepdims=True)
    
    # ê°€ì¤‘ì¹˜ ì ìš© í›„ ê²°í•©
    fused_features = np.concatenate([
        resnet_norm * resnet_weight,
        pos_norm * pos_weight
    ], axis=1)
    
    return fused_features

def evaluate_clustering(features, clusters):
    """í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ í‰ê°€"""
    if len(set(clusters)) <= 1:  # í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œ ì´í•˜ë©´ í‰ê°€ ë¶ˆê°€
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': 0}
    
    # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì œê±°
    valid_mask = clusters != -1
    if np.sum(valid_mask) < 2:
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': 0}
    
    valid_features = features[valid_mask]
    valid_clusters = clusters[valid_mask]
    
    if len(set(valid_clusters)) <= 1:
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': len(set(valid_clusters))}
    
    try:
        silhouette = silhouette_score(valid_features, valid_clusters)
        calinski = calinski_harabasz_score(valid_features, valid_clusters)
        davies = davies_bouldin_score(valid_features, valid_clusters)
        n_clusters = len(set(valid_clusters))
        
        return {
            'silhouette': silhouette,
            'calinski': calinski,
            'davies': davies,
            'n_clusters': n_clusters
        }
    except:
        return {'silhouette': -1, 'calinski': 0, 'davies': float('inf'), 'n_clusters': 0}

def perform_clustering_with_eval(features):
    """UMAP + HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ë° í‰ê°€"""
    # íŠ¹ì§• ì •ê·œí™”
    features_scaled = StandardScaler().fit_transform(features)
    
    # UMAP ì°¨ì› ì¶•ì†Œ
    reducer = umap.UMAP(
        n_neighbors=10,
        min_dist=0.001,
        n_components=2,
        random_state=42,
        metric='euclidean'
    )
    umap_embedded = reducer.fit_transform(features_scaled)
    
    # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=70,
        min_samples=3,
        cluster_selection_epsilon=0.05,
        metric='euclidean'
    )
    clusters = clusterer.fit_predict(umap_embedded)
    
    # ì„±ëŠ¥ í‰ê°€
    metrics = evaluate_clustering(umap_embedded, clusters)
    
    return umap_embedded, clusters, metrics

# ============================================================================
#  ê°€ì¤‘ì¹˜ ì¡°í•© ìµœì í™” ë° ì‹œê°í™”
# ============================================================================

def comprehensive_weight_optimization(resnet_features, pos_features, save_dir='.'):
    """
    ê°€ì¤‘ì¹˜ ì¡°í•©ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ë° UMAP ì‹œê°í™”
    """
    print(" Starting weight optimization...")
    
    # 2ì°¨ì› ê°€ì¤‘ì¹˜ ì¡°í•© ì •ì˜
    resnet_weights = [0.1, 0.5, 1.0, 2.0, 5.0]
    pos_weights = [1.0, 10.0, 50.0, 100.0, 500.0]  
    
    print(f"Testing {len(resnet_weights)} Ã— {len(pos_weights)} = {len(resnet_weights) * len(pos_weights)} combinations")
    
    # ê²°ê³¼ ì €ì¥
    all_results = []
    
    combinations = list(product(resnet_weights, pos_weights))
    
    for i, (rw, pw) in enumerate(tqdm(combinations, desc="Testing weight combinations")):
        try:
            # íŠ¹ì§• ìœµí•©
            fused_features = fuse_features(resnet_features, pos_features,
                                         resnet_weight=rw, pos_weight=pw)
            
            # í´ëŸ¬ìŠ¤í„°ë§ ë° í‰ê°€
            umap_embedded, clusters, metrics = perform_clustering_with_eval(fused_features)
            
            result = {
                'combination_idx': i,
                'resnet_weight': rw,
                'pos_weight': pw,  
                'silhouette': metrics['silhouette'],
                'calinski': metrics['calinski'],
                'davies': metrics['davies'],
                'n_clusters': metrics['n_clusters'],
                'umap_embedded': umap_embedded,
                'clusters': clusters
            }
            
            all_results.append(result)
            
        except Exception as e:
            print(f"Error with combination ({rw}, {pw}): {e}")
            continue
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    results_df = pd.DataFrame([
        {k: v for k, v in r.items() if k not in ['umap_embedded', 'clusters']}
        for r in all_results
    ])
    
    # ê²°ê³¼ ì €ì¥
    results_df.to_csv(os.path.join(save_dir, 'weight_optimization_results.csv'), index=False)
    
    # ìµœì  ì¡°í•©ë“¤ ì„ íƒ
    top_results = sorted(all_results, key=lambda x: x['silhouette'], reverse=True)[:20]
    
    print(f"\n Top 20 combinations found!")
    print("="*60)
    for i, result in enumerate(top_results[:10]):
        print(f"{i+1:2d}. ResNet:{result['resnet_weight']:4.1f} | Pos:{result['pos_weight']:5.0f} | "
              f"Sil:{result['silhouette']:.3f} | Clusters:{result['n_clusters']:2d}")
    
    #  ì¢…í•© ì‹œê°í™”
    plot_comprehensive_results(top_results, results_df, save_dir)
    
    return top_results, results_df

def plot_comprehensive_results(top_results, results_df, save_dir):
    """ì¢…í•© ê²°ê³¼ ì‹œê°í™”"""
    
    # 1.  ìƒìœ„ 20ê°œ ì¡°í•©ì˜ UMAP ê²°ê³¼ (4x5 ê·¸ë¦¬ë“œ)
    fig, axes = plt.subplots(4, 5, figsize=(25, 20))
    axes = axes.flatten()
    
    for i, result in enumerate(top_results):
        ax = axes[i]
        
        umap_embedded = result['umap_embedded']
        clusters = result['clusters']
        
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
        noise_mask = clusters == -1
        if np.any(noise_mask):
            ax.scatter(umap_embedded[noise_mask, 0], umap_embedded[noise_mask, 1],
                      c='lightgray', alpha=0.3, s=8, label='Noise')
        
        # í´ëŸ¬ìŠ¤í„° í¬ì¸íŠ¸
        cluster_mask = ~noise_mask
        if np.any(cluster_mask):
            scatter = ax.scatter(umap_embedded[cluster_mask, 0], umap_embedded[cluster_mask, 1],
                               c=clusters[cluster_mask], cmap='tab20', alpha=0.8, s=12)
        
        ax.set_title(f'#{i+1}: ({result["resnet_weight"]:.1f}, {result["pos_weight"]:.0f})\n'
                    f'Sil: {result["silhouette"]:.3f} | Clusters: {result["n_clusters"]}', 
                    fontsize=10)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.grid(True, alpha=0.3)
    
    plt.suptitle(' Top 20 Weight Combinations - UMAP Clustering Results', fontsize=16, y=0.95)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'top20_umap_results.png'), dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # ì‹¤ë£¨ì—£ ì ìˆ˜ ë¶„í¬
    axes[0, 0].hist(results_df['silhouette'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].axvline(results_df['silhouette'].mean(), color='red', linestyle='--', 
                       label=f'Mean: {results_df["silhouette"].mean():.3f}')
    axes[0, 0].set_title('Silhouette Score Distribution')
    axes[0, 0].set_xlabel('Silhouette Score')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶„í¬
    max_clusters = results_df['n_clusters'].max()
    axes[0, 1].hist(results_df['n_clusters'], bins=range(0, max_clusters+2), 
                    alpha=0.7, color='lightgreen', edgecolor='black')
    axes[0, 1].set_title('Number of Clusters Distribution')
    axes[0, 1].set_xlabel('Number of Clusters')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].grid(True, alpha=0.3)
    
    # ResNet ê°€ì¤‘ì¹˜ë³„ ì„±ëŠ¥
    resnet_perf = results_df.groupby('resnet_weight')['silhouette'].agg(['mean', 'std']).reset_index()
    axes[0, 2].bar(resnet_perf['resnet_weight'], resnet_perf['mean'], 
                   yerr=resnet_perf['std'], capsize=5, alpha=0.7, color='orange')
    axes[0, 2].set_title('Performance by ResNet Weight')
    axes[0, 2].set_xlabel('ResNet Weight')
    axes[0, 2].set_ylabel('Mean Silhouette Score')
    axes[0, 2].grid(True, alpha=0.3)
    
    # Positional ê°€ì¤‘ì¹˜ë³„ ì„±ëŠ¥
    pos_perf = results_df.groupby('pos_weight')['silhouette'].agg(['mean', 'std']).reset_index()
    axes[1, 0].bar(range(len(pos_perf)), pos_perf['mean'], 
                   yerr=pos_perf['std'], capsize=5, alpha=0.7, color='red')
    axes[1, 0].set_title('Performance by Positional Weight')
    axes[1, 0].set_xlabel('Positional Weight')
    axes[1, 0].set_ylabel('Mean Silhouette Score')
    axes[1, 0].set_xticks(range(len(pos_perf)))
    axes[1, 0].set_xticklabels([f'{w:.0f}' for w in pos_perf['pos_weight']], rotation=45)
    axes[1, 0].grid(True, alpha=0.3)
    
    # ê°€ì¤‘ì¹˜ ì¡°í•© íˆíŠ¸ë§µ (ResNet vs Pos)
    pivot_data = results_df.groupby(['resnet_weight', 'pos_weight'])['silhouette'].mean().unstack()
    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='viridis', ax=axes[1, 1])
    axes[1, 1].set_title('Weight Combination Heatmap\n(ResNet vs Positional)')
    axes[1, 1].set_xlabel('Positional Weight')
    axes[1, 1].set_ylabel('ResNet Weight')
    
    # ë¹ˆ subplot ì œê±°
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'weight_optimization_analysis.png'), dpi=300, bbox_inches='tight')
    plt.show()

# ============================================================================
#  MAIN ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¬ Histology-Specialized Clustering with Weight Optimization")
    print("="*60)
    
    # ë°ì´í„° ë¡œë“œ
    print(" Loading data...")
    df = pd.read_csv('dataset.csv')
    
    #  ì¡°ì§ 3ë²ˆ ì œì™¸ (1, 2, 4ë²ˆë§Œ ì‚¬ìš©)
    print(f"Original data: {len(df)} samples")
    df = df[df['tissue_index'].isin([1, 2, 4])]
    print(f"After filtering (tissues 1,2,4 only): {len(df)} samples")
    print(f"Tissue distribution: {df['tissue_index'].value_counts().sort_index()}")
    
    # Transform ì„¤ì •
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Dataset & DataLoader
    dataset = TissueDataset('train', df, transform)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)
    
    #  1. Positional Encoding ìƒì„±
    print("\nStep 1: Creating positional encoding...")
    pos_features = create_positional_features(df, pos_dim=64)
    
    # 2. ì¡°ì§ë³‘ë¦¬í•™ íŠ¹í™” ResNet íŠ¹ì§• ì¶”ì¶œ
    print("\n Step 2: Extracting histology-specialized features...")
    extractor = HistologyFeatureExtractor(model_path='weights/tenpercent_resnet18.ckpt')
    resnet_features, names = extractor.extract_features(dataloader)
    
    # 3. ê°€ì¤‘ì¹˜ ìµœì í™” ë° ì‹œê°í™”
    print("\n Step 3: Weight optimization...")
    top_results, results_df = comprehensive_weight_optimization(
        resnet_features, pos_features, save_dir='.'
    )
    
    #  4. ìµœì  ê²°ê³¼ë¡œ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
    print("\n Step 4: Final clustering with best weights...")
    best_result = top_results[0]
    
    #  5. ê²°ê³¼ ì €ì¥
    print("\n Step 5: Saving final results...")
    final_results_df = pd.DataFrame({
        'image_name': names,
        'cluster': best_result['clusters'],
        'array_row': df['array_row'].values,
        'array_col': df['array_col'].values,
        'tissue_index': df['tissue_index'].values,
        'umap_x': best_result['umap_embedded'][:, 0],
        'umap_y': best_result['umap_embedded'][:, 1]
    })
    
    final_results_df.to_csv('histology_clustering_results.csv', index=False)
    
    # ìµœì¢… í†µê³„
    cluster_stats = final_results_df['cluster'].value_counts().sort_index()
    print(f"\n Final Clustering Statistics (Best Combination):")
    print(f"   Best weights: ResNet={best_result['resnet_weight']:.1f}, "
          f"Pos={best_result['pos_weight']:.0f}")
    print(f"   Silhouette Score: {best_result['silhouette']:.3f}")
    print(f"   Number of clusters: {best_result['n_clusters']}")
    print("   Cluster distribution:")
    for cluster_id, count in cluster_stats.items():
        if cluster_id == -1:
            print(f"     Noise: {count} samples")
        else:
            print(f"     Cluster {cluster_id}: {count} samples")
    
    print("\nğŸ‰ Complete! Results saved:")
    print("   - histology_clustering_results.csv")
    print("   - weight_optimization_results.csv") 
    print("   - top20_umap_results.png")
    print("   - weight_optimization_analysis.png")

# ============================================================================
#  ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ============================================================================

def run_quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì†Œìˆ˜ ì¡°í•©ë§Œ)"""
    print(" Running Quick Test...")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('dataset.csv')
    
    #  ì¡°ì§ 3ë²ˆ ì œì™¸ (1, 2, 4ë²ˆë§Œ ì‚¬ìš©)  
    df = df[df['tissue_index'].isin([1, 2, 4])]
    print(f"Using tissues 1, 2, 4 only: {len(df)} samples")
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    dataset = TissueDataset('train', df, transform)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)
    
    # íŠ¹ì§• ì¶”ì¶œ
    pos_features = create_positional_features(df, pos_dim=64)
    extractor = HistologyFeatureExtractor(model_path='weights/tenpercent_resnet18.ckpt')
    resnet_features, names = extractor.extract_features(dataloader)
    
    # ì œí•œëœ ì¡°í•© í…ŒìŠ¤íŠ¸
    quick_combinations = [
        (0.5, 50), (1.0, 100), (2.0, 100), (1.0, 500)
    ]
    
    results = []
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    for i, (rw, pw) in enumerate(quick_combinations):
        print(f"Testing combination {i+1}/4: ({rw}, {pw})")
        
        fused_features = fuse_features(resnet_features, pos_features,
                                     resnet_weight=rw, pos_weight=pw)
        
        umap_embedded, clusters, metrics = perform_clustering_with_eval(fused_features)
        
        # ì‹œê°í™”
        ax = axes[i]
        noise_mask = clusters == -1
        if np.any(noise_mask):
            ax.scatter(umap_embedded[noise_mask, 0], umap_embedded[noise_mask, 1],
                      c='lightgray', alpha=0.3, s=10)
        
        cluster_mask = ~noise_mask
        if np.any(cluster_mask):
            ax.scatter(umap_embedded[cluster_mask, 0], umap_embedded[cluster_mask, 1],
                      c=clusters[cluster_mask], cmap='tab20', alpha=0.8, s=15)
        
        ax.set_title(f'Weights: ({rw}, {pw})\nSil: {metrics["silhouette"]:.3f} | '
                    f'Clusters: {metrics["n_clusters"]}')
        ax.set_xticks([])
        ax.set_yticks([])
        
        results.append((rw, pw, metrics['silhouette'], metrics['n_clusters']))
    
    plt.tight_layout()
    plt.savefig('quick_test_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n Quick Test Results:")
    results.sort(key=lambda x: x[2], reverse=True)
    for i, (rw, pw, sil, n_clust) in enumerate(results):
        print(f"{i+1}. ({rw:3.1f}, {pw:3.0f}) | Sil: {sil:.3f} | Clusters: {n_clust}")

if __name__ == "__main__":
    # ì‹¤í–‰ ì˜µì…˜ ì„ íƒ
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'quick':
        run_quick_test()
    else:
        # ê¸°ë³¸ ì‹¤í–‰
        main()

=================
def show_gallery(d, max_imgs=200, size=224):
    imgs = list_images(d)[:max_imgs]
    html = ["<div style='display:flex;flex-wrap:wrap;gap:6px'>"]
    for p in imgs:
        html.append(f"<img src='{p.as_posix()}' width='{size}' height='{size}'/>")
    html.append("</div>")
    display(HTML("".join(html)))

for d in clusters:
    print(d.name)
    show_gallery(d, max_imgs=300, size=224) 
